---
title: "p8105_hw6_ksf2129"
author: "Kimia Faroughi"
date: "2025-12-02"
output: github_document
---

Load packages

```{r, message = FALSE}
library(tidyverse)
library(p8105.datasets)
library(modelr)

set.seed(1)
```

## Problem 1

Read in the data

```{r, message = FALSE}
homicides_df = 
  read_csv("data/homicide-data.csv")
```

Clean data and create variables

```{r, warning = FALSE}
homicides_df =
  homicides_df |> 
  #create city_state var, binary var whether homicide is solved, convert age to numeric
  mutate(
    city_state = str_c(city, state, sep = ", "),
    solved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      TRUE ~ "solved"
    ),
    victim_age = as.numeric(victim_age)
  ) |> 
  filter(city_state != "Tulsa, AL", 
         city_state != "Dallas, TX", 
         city_state != "Phoenix, AZ", 
         city_state != "Kansas City, MO",
         victim_race == "White" | victim_race == "Black")
```

Fit logistic regression model for Baltimore

```{r}
#create dataset with just Baltimore, MD
baltimore_df = 
  homicides_df |> 
  filter(city_state == "Baltimore, MD") |> 
  mutate(
    solved = factor(solved, levels = c("unsolved", "solved")) #convert outcome to factor, with unsolved as the reference
  )
  
fit_baltimore = glm(solved ~ victim_age + victim_sex + victim_race, data = baltimore_df, family = binomial())
```

Clean up output

```{r}
fit_baltimore |> 
  broom::tidy() |> 
  filter(term == "victim_sexMale") |> 
  mutate(
    OR = exp(estimate),
    lower_CI = exp(estimate - (1.96*std.error)),
    upper_CI = exp(estimate + (1.96*std.error)),
    term = str_replace(term, "victim_sex", "Victim Sex: ")
    ) |>
  select(term, OR, lower_CI, upper_CI) |> 
  knitr::kable(digits = 3)
```

Do this for all cities

```{r}
#turn outcome into factor var
homicides_df = 
  homicides_df |> 
  mutate(
    solved = factor(solved, levels = c("unsolved", "solved")) #convert outcome to factor, with unsolved as the reference
  )

#fit regression model to each city
fit_allcities =
  homicides_df |> 
  nest(data = -city_state) |> 
  mutate(
    fits = map(data, \(df) glm(solved ~ victim_age + victim_sex + victim_race, data = df, family = binomial())),
    glm_results = map(fits, broom::tidy)
  ) |> 
  select(city_state, glm_results) |> 
  unnest(glm_results)

#create df with OR and CI for each city
all_cities = 
  fit_allcities |> 
    filter(term == "victim_sexMale") |> 
    mutate(
      OR = exp(estimate),
      lower_CI = exp(estimate - (1.96*std.error)),
      upper_CI = exp(estimate + (1.96*std.error))
      ) |>
    select(city_state, OR, lower_CI, upper_CI)
```

Plot estimated ORs and CIs for each city

```{r, fig.width = 9, fig.height = 6}
all_cities |> 
  mutate(
    city_state = fct_reorder(city_state, OR)
  ) |> 
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = lower_CI, ymax = upper_CI)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 1)) +
  labs(
      x = "City",
      y = "Estimated OR of solved homicides\n (in males compared to females) and 95% CIs",
    ) 
```

This plot shows that many of the cities' estimates of the odds of a solved homicide is smaller if the victim is male compared to if the victim is female, since many of the estimated ORs are less than 1. However, some of these ORs have a corresponding 95% CI that includes the null value 1, such as Long Beach, San Bernardino, San Francisco, Sacramento, etc. This means that the apparent observation that males have a lower odds of solved homicides compared to females in such cities, using a 5% significance level and adjusting for victim age and race, can be ruled out due to chance. The same is true for all estimated ORs that are greater than 1 (Nashville, Fresno, Stockton, and Albuquerque), whose point estimates show that males have higher odds of a solved homicide compared to females, but the 95% CI includes the null value 1. Oklahoma City, Tulsa, Atlanta, and Richmond have an OR approximately equal to 1, indicating that the odds of a solved homicide is about the same for males compared to females. Finally, cities such as New York, Baton Rouge, Omaha, Cincinnati, Chicago, etc. have an estimate OR < 1 with the 95% CIs contained less than 1 as well, meaning the observation that the estimated odds of a solved homicide is smaller in males compared to females cannot be ruled out due to random chance, using a significance level of 5%, adjusting for victim age and race. 

## Problem 2

Load dataset 

```{r}
data("weather_df")
```

Bootstrapping

```{r, warning = FALSE}
#generate estimates
weather_bootstrap_results = 
  weather_df |> 
  bootstrap(n = 5000) |> #always start small and then change as needed 
  mutate(df = map(strap, as_tibble), 
        fits = map(df, \(df) lm(tmax ~ tmin + prcp, data = df)), 
        parameters = map(fits, broom::tidy), #beta1 and beta2 estimates 
        r_squared = map(fits, broom::glance) #r^2 
        ) |> 
  select(.id, parameters, r_squared) |> 
  unnest(parameters, r_squared)

weather_bootstrap_estimates = 
  weather_bootstrap_results |> 
  select(.id, term, estimate, std.error, r.squared) |> 
  pivot_wider(
    values_from = estimate:std.error,
    names_from = term
  ) |> 
  janitor::clean_names() |> 
  select(-estimate_intercept, -std_error_intercept) |> 
  mutate(
    beta1_beta2 = estimate_tmin / estimate_prcp
  )
```

Plot estimates

```{r}
#r squared
weather_bootstrap_estimates |> 
  ggplot(aes(x = r_squared)) +
  geom_density()

#beta1 / beta2
weather_bootstrap_estimates |> 
  ggplot(aes(x = beta1_beta2)) +
  geom_density()
```

The plot of the r_squared estimates across the bootstrap sample shows a relatively symmetric distribution of values. The plot of the beta1/beta2 estimates, where beta1 is the estimated coefficient in the linear regression model for `tmin` and beta2 is the estimated coefficient for `prcp`, shows a much more left-skewed distribution.

95% confidence interval for r squared

```{r}
#r squared
weather_bootstrap_estimates |> 
  summarize(
    ci_lower = quantile(r_squared, 0.025),
    ci_upper = quantile(r_squared, 0.975)
  ) |> 
  knitr::kable()
```

95% confidence interval for beta1 / beta2

```{r}
#beta1 / beta2
weather_bootstrap_estimates |> 
  summarize(
    ci_lower = quantile(beta1_beta2, 0.025),
    ci_upper = quantile(beta1_beta2, 0.975)
  ) |> 
  knitr::kable()
```

## Problem 3

Load and clean data

```{r, message = FALSE}
birthweight_df = 
  read_csv("data/birthweight.csv") |> 
  janitor::clean_names() |> 
  mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    malform = as.factor(malform),
    mrace = as.factor(mrace),
  )

sum(is.na(birthweight_df))
```

There are no NAs in this dataset.

I propose a regression model with `bwt` (birthweight of the baby in grams) as the outcome and `babysex` (baby's sex), `delwt` (mother's weight at delivery in pounds), `gaweeks` (gestational age in weeks), `malform` (presence of malformations that could affect weight), and `smoken` (average number of cigarettes smoked per day during pregnancy) as the predictors. This is based off the hypothesis that the mother's weight at delivery and the gestational age strongly inform the birthweight of the baby, with the sex of the baby, presence of malformations, and whether the mother smoked cigarettes during pregnancy also impacting the baby's birthweight. I created this model by considering potential factors that underlie birthweight, while avoiding including any potentially correlated predictors. 

Plot of model residuals against fitted values

```{r}
#run model
bwt_lm = lm(bwt ~ babysex + delwt + gaweeks + malform + smoken, data = birthweight_df)

#plot
birthweight_df |> 
  add_predictions(bwt_lm) |> 
  add_residuals(bwt_lm) |> 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .5)
```

Compare model to two others in terms of cross-validation prediction error

```{r}
cv_df =
  crossv_mc(birthweight_df, n = 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
```

```{r}
cv_df =
  cv_df |> 
  mutate(
    bwt_lm = map(train, \(df) lm(bwt ~ babysex + delwt + gaweeks + malform + smoken, data = birthweight_df)),
    main_effects = map(train, \(df) lm(bwt ~ blength + gaweeks, data = birthweight_df)),
    interaction_lm = 
      map(train, \(df) lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + 
      bhead*blength*babysex, data = birthweight_df))
  ) |> 
  mutate(
    rmse_bwt_lm = map2_dbl(bwt_lm, test, rmse),
    rmse_main_effects = map2_dbl(main_effects, test, rmse),
    rmse_interaction_lm = map2_dbl(interaction_lm, test, rmse)
  )
```

Plot prediction error distribution for each model

```{r}
cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) |> 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin()
```

Based on the distribution of prediction errors for each model, the model I created is clearly the least appropriate model, with the highest root mean square error distribution. The model with just main effects shows a great deal of improvement in predictive accuracy, and the model with the interaction terms shows even more improvement. Therefore, the best model seems to be the model with the interaction terms, with the next best being the model with just main effects, and the worst model being the one that I proposed. 